{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "14a8ebd9-b79d-4a71-9854-09eecd0f4df0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using ORDERS column: ORDERS\n",
      "Parsing orders (this may take a while)...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e90fd1d2f5e04cb7a741d0ebb5243104",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1414410 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample parsed items (first 8):\n",
      "0 ['Order Memo Not Paid', '10 pc Grilled Wings Combo', '8 pc Grilled Wings Combo', '8 pc Spicy Wings Combo']\n",
      "1 ['Ranch Dip - Regular', '50 pc Grilled Wings', 'Regular Buffalo Fries', 'Order Memo Paid']\n",
      "2 ['20pc Spicy Feast Deal', 'Order Memo Paid']\n",
      "3 ['Order Memo Item', 'Order Memo Paid', '20 pc Grilled Wings', 'Order Memo ASAP', 'Ranch Dip - Regular', 'Order Blankline 2', 'Order Blankline 1']\n",
      "4 ['Order Blankline 2', '6 pc Grilled Wings Combo', '8 pc Grilled Wings Combo', 'Order Blankline 1', 'Order Memo ASAP', 'Order Memo Item', 'Order Memo Paid']\n",
      "5 ['Order Memo Paid', '10 pc Grilled Wings Combo']\n",
      "6 ['10 pc Grilled Wings', 'Order Memo Paid', 'Ranch Dip - Regular']\n",
      "7 ['Order Memo Paid', '20pc Spicy Feast Deal']\n",
      "Orders after parsing (rows): 1414410\n",
      "Building frequency & co-occurrence (noise filtered)...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e21c868f8bbb4d008eed3fed3563c562",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1414410 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique (non-noise) items: 138\n",
      "Top 20 popular items (display -> freq):\n",
      " Ranch Dip - Regular -> 302870\n",
      " 20pc Spicy Feast Deal -> 267974\n",
      " 10 pc Grilled Wings Combo -> 166664\n",
      " 6 pc Grilled Wings Combo -> 117894\n",
      " 8 pc Grilled Wings Combo -> 117595\n",
      " Regular Buffalo Fries -> 100141\n",
      " 2 pc Crispy Strips -> 84162\n",
      " Ranch Dip - Large -> 80610\n",
      " 6 pc Spicy Wings Combo -> 72234\n",
      " 10 pc Grilled Wings -> 67043\n",
      " Large Buffalo Fries -> 59962\n",
      " 8 pc Spicy Wings Combo -> 59308\n",
      " 10 pc Spicy Wings -> 59039\n",
      " Fried Corn - Regular -> 58584\n",
      " Chicken Sub Combo -> 58169\n",
      " 10 pc Spicy Wings Combo -> 57397\n",
      " Flavor Platter -> 55667\n",
      " 3 pc Crispy Strips Combo -> 54478\n",
      " Chicken Sub -> 45257\n",
      " 15 pc Grilled Wings Combo -> 43452\n",
      "Computing PPMI...\n",
      "Detected test item columns: ['item1', 'item2', 'item3']\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e163add23332498e90d61e935767e782",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrote recommendations_output.csv\n",
      "Sample recommendations (first 8 rows):\n",
      "     RECOMMENDATION 1          RECOMMENDATION 2          RECOMMENDATION 3\n",
      " Cheese Dip - Regular      Fried Corn - Regular     Regular Buffalo Fries\n",
      " Cheese Dip - Regular                20 Oz Soda      Fried Corn - Regular\n",
      "Regular Buffalo Fries Blue Cheese Dip - Regular    Voodoo Fries - Regular\n",
      "  Large Buffalo Fries Blue Cheese Dip - Regular     Regular Buffalo Fries\n",
      "  Large Buffalo Fries   Blue Cheese Dip - Large Blue Cheese Dip - Regular\n",
      "   2 pc Crispy Strips         Add 5 Spicy Wings        4 pc Crispy Strips\n",
      "Regular Buffalo Fries Blue Cheese Dip - Regular      Fried Corn - Regular\n",
      "Regular Buffalo Fries      Fried Corn - Regular       Large Buffalo Fries\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Robust recommender (PPMI + co-occurrence) with noise filtering.\n",
    "Requirements: pandas, numpy, tqdm\n",
    "pip install pandas numpy tqdm\n",
    "\"\"\"\n",
    "\n",
    "import os, re, json, math\n",
    "from collections import Counter, defaultdict\n",
    "from itertools import combinations\n",
    "from tqdm.auto import tqdm\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# ---------------- CONFIG ----------------\n",
    "ORDER_CSV = \"order_data.csv\"\n",
    "TEST_CSV = \"test_data_question.csv\"\n",
    "OUTPUT_CSV = \"recommendations_output.csv\"\n",
    "\n",
    "TOP_K = 3\n",
    "VERBOSE = True\n",
    "\n",
    "# thresholds/tunables\n",
    "MIN_CO_OCCURRENCE = 1         # keep all pairs >= this\n",
    "PPMI_EPS = 1e-12\n",
    "GAMMA_POP = 0.3               # popularity fallback weight\n",
    "ALPHA_PPMI = 1.0              # weight for ppmi*strength\n",
    "# ---------------------------------------\n",
    "\n",
    "def vprint(*a, **k):\n",
    "    if VERBOSE:\n",
    "        print(*a, **k)\n",
    "\n",
    "# ---------------- parsing utilities ----------------\n",
    "def normalize_display(s):\n",
    "    if s is None: return None\n",
    "    s2 = str(s).strip()\n",
    "    if not s2: return None\n",
    "    s2 = re.sub(r'\\s+', ' ', s2)           # collapse whitespace\n",
    "    s2 = s2.replace('™','').replace('®','')\n",
    "    return s2\n",
    "\n",
    "def lower_norm(s):\n",
    "    ns = normalize_display(s)\n",
    "    return ns.lower() if ns is not None else None\n",
    "\n",
    "def try_json_extract_item_names(raw):\n",
    "    \"\"\"\n",
    "    Try to parse JSON and find item display strings in common keys.\n",
    "    Returns list or None.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        obj = json.loads(raw)\n",
    "    except Exception:w\n",
    "        return None\n",
    "    names = []\n",
    "    # patterns found in dataset: orders -> item_details -> item_name\n",
    "    def collect(o):\n",
    "        if isinstance(o, dict):\n",
    "            # check keys with lists of items\n",
    "            for k,v in o.items():\n",
    "                if isinstance(v, list):\n",
    "                    for x in v:\n",
    "                        collect(x)\n",
    "                elif isinstance(v, dict):\n",
    "                    collect(v)\n",
    "                elif isinstance(v, str):\n",
    "                    # try keys that look like item names\n",
    "                    if k.lower() in ('item_name','name','title','product_name','description'):\n",
    "                        names.append(v.strip())\n",
    "            # also inspect keys 'orders','items','line_items'\n",
    "            if 'orders' in o and isinstance(o['orders'], list):\n",
    "                for it in o['orders']:\n",
    "                    collect(it)\n",
    "            if 'item_details' in o and isinstance(o['item_details'], list):\n",
    "                for it in o['item_details']:\n",
    "                    collect(it)\n",
    "        elif isinstance(o, list):\n",
    "            for e in o:\n",
    "                collect(e)\n",
    "        elif isinstance(o, str):\n",
    "            names.append(o.strip())\n",
    "    collect(obj)\n",
    "    # dedupe and return\n",
    "    out = []\n",
    "    for n in names:\n",
    "        if n and n not in out:\n",
    "            out.append(n)\n",
    "    return out if out else None\n",
    "\n",
    "def regex_extract_item_names(s):\n",
    "    # specific patterns: \"item_name\": \"XYZ\"\n",
    "    if not isinstance(s, str): return None\n",
    "    names = []\n",
    "    for m in re.finditer(r'\"item_name\"\\s*:\\s*\"([^\"]+)\"', s, flags=re.IGNORECASE):\n",
    "        names.append(m.group(1).strip())\n",
    "    for m in re.finditer(r\"'item_name'\\s*:\\s*'([^']+)'\", s, flags=re.IGNORECASE):\n",
    "        names.append(m.group(1).strip())\n",
    "    return names if names else None\n",
    "\n",
    "def fallback_split(s):\n",
    "    # split on common delimiters\n",
    "    parts = re.split(r'\\s*\\|\\|\\s*|\\s*;;\\s*|\\s*\\|\\s*|\\s*;\\s*|\\s*\\/\\s*|\\s*\\n\\s*|\\s*,\\s*', s)\n",
    "    parts = [p.strip() for p in parts if p and p.strip()]\n",
    "    return parts\n",
    "\n",
    "def parse_order_field(field):\n",
    "    \"\"\"\n",
    "    Return a list of display strings extracted from ORDERS column for a single row.\n",
    "    \"\"\"\n",
    "    if field is None:\n",
    "        return []\n",
    "    raw = str(field).strip()\n",
    "    if raw == \"\":\n",
    "        return []\n",
    "    # Try JSON structured extraction\n",
    "    j = try_json_extract_item_names(raw)\n",
    "    if j: return [normalize_display(x) for x in j if normalize_display(x)]\n",
    "    # regex\n",
    "    r = regex_extract_item_names(raw)\n",
    "    if r: return [normalize_display(x) for x in r if normalize_display(x)]\n",
    "    # fallback splits\n",
    "    parts = fallback_split(raw)\n",
    "    if len(parts) >= 2:\n",
    "        return [normalize_display(x) for x in parts if normalize_display(x)]\n",
    "    # final fallback: return full string\n",
    "    return [normalize_display(raw)]\n",
    "\n",
    "# ---------------- noise detection ----------------\n",
    "NOISE_PATTERNS = [\n",
    "    r'\\bmemo\\b', r'blankline', r'\\bblank\\b', r'\\bpaid\\b', r'\\bpayment\\b',\n",
    "    r'\\bnote\\b', r'\\binstruction\\b', r'\\bphone\\b', r'\\baddress\\b', r'\\bsubtotal\\b',\n",
    "    r'\\btotal\\b', r'\\btip\\b', r'\\bcoupon\\b', r'\\bdiscount\\b', r'\\bpromo\\b',\n",
    "    r'\\bvoid\\b', r'\\bcancelled\\b', r'\\bcanceled\\b', r'\\border\\s*memo\\b',\n",
    "    r'\\border\\s*blank', r'order blankline'\n",
    "]\n",
    "NOISE_RE = re.compile('|'.join(NOISE_PATTERNS), flags=re.IGNORECASE)\n",
    "\n",
    "def is_noise_item(display_name):\n",
    "    if display_name is None: return True\n",
    "    s = display_name.strip()\n",
    "    if s == \"\": return True\n",
    "    s_low = s.lower()\n",
    "    # rule: explicitly match waste patterns\n",
    "    if NOISE_RE.search(s_low):\n",
    "        return True\n",
    "    # remove entries like \"Order Memo Item\" etc\n",
    "    if len(s) < 3:\n",
    "        return True\n",
    "    # if string is mostly punctuation or digits\n",
    "    if re.fullmatch(r'[\\W_]+', s):\n",
    "        return True\n",
    "    # if text is something like 'Blank', 'N/A', 'NaN'\n",
    "    if s_low in {'na','n/a','nan','missing','none','null', '0 pc'}:\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "# ---------------- load data ----------------\n",
    "if not os.path.exists(ORDER_CSV):\n",
    "    raise FileNotFoundError(f\"{ORDER_CSV} not found. Put it in the current folder.\")\n",
    "\n",
    "orders_df = pd.read_csv(ORDER_CSV, dtype=str, keep_default_na=False, na_values=[\"\"])\n",
    "if 'ORDERS' not in orders_df.columns:\n",
    "    # try to find column\n",
    "    found = None\n",
    "    for c in orders_df.columns:\n",
    "        if 'order' in c.lower():\n",
    "            found = c; break\n",
    "    if not found:\n",
    "        raise ValueError(\"Could not find ORDERS column in order_data.csv\")\n",
    "    orders_col = found\n",
    "else:\n",
    "    orders_col = 'ORDERS'\n",
    "\n",
    "vprint(\"Using ORDERS column:\", orders_col)\n",
    "\n",
    "# parse\n",
    "vprint(\"Parsing orders (this may take a while)...\")\n",
    "tqdm.pandas()\n",
    "orders_df['items_raw'] = orders_df[orders_col].progress_map(parse_order_field)\n",
    "\n",
    "# display some parsed examples\n",
    "vprint(\"Sample parsed items (first 8):\")\n",
    "for i, r in enumerate(orders_df['items_raw'].head(8)):\n",
    "    vprint(i, r)\n",
    "\n",
    "# normalize and build mapping (normalized key = lower-case stripped)\n",
    "norm_to_display_counts = defaultdict(Counter)\n",
    "def make_norm_list(raw_list):\n",
    "    out = []\n",
    "    for s in raw_list:\n",
    "        if s is None: continue\n",
    "        disp = str(s).strip()\n",
    "        if disp == \"\": continue\n",
    "        # keep display, and normalized key\n",
    "        norm = disp.lower()\n",
    "        out.append(norm)\n",
    "        norm_to_display_counts[norm][disp] += 1\n",
    "    # dedupe preserving order\n",
    "    seen = set(); uniq=[]\n",
    "    for x in out:\n",
    "        if x not in seen:\n",
    "            seen.add(x); uniq.append(x)\n",
    "    return uniq\n",
    "\n",
    "orders_df['items_norm'] = orders_df['items_raw'].map(make_norm_list)\n",
    "# remove rows with no items\n",
    "orders_df = orders_df[orders_df['items_norm'].map(len) > 0].reset_index(drop=True)\n",
    "vprint(\"Orders after parsing (rows):\", len(orders_df))\n",
    "\n",
    "# ---------- filter out noisy item norms before counting ----------\n",
    "vprint(\"Building frequency & co-occurrence (noise filtered)...\")\n",
    "item_freq = Counter()\n",
    "co_counts = defaultdict(Counter)\n",
    "\n",
    "for items in tqdm(orders_df['items_norm'], total=len(orders_df)):\n",
    "    # filter noise and dedupe\n",
    "    filtered = []\n",
    "    for n in items:\n",
    "        disp = norm_to_display_counts[n].most_common(1)[0][0] if n in norm_to_display_counts else n\n",
    "        if not is_noise_item(disp):\n",
    "            filtered.append(n)\n",
    "    if len(filtered) == 0:\n",
    "        continue\n",
    "    unique_items = list(dict.fromkeys(filtered))\n",
    "    for it in unique_items:\n",
    "        item_freq[it] += 1\n",
    "    for a,b in combinations(unique_items, 2):\n",
    "        co_counts[a][b] += 1\n",
    "        co_counts[b][a] += 1\n",
    "\n",
    "if len(item_freq) == 0:\n",
    "    raise RuntimeError(\"No non-noise items found after filtering. Try lowering noise rules or inspect parsed sample.\")\n",
    "\n",
    "vprint(\"Unique (non-noise) items:\", len(item_freq))\n",
    "vprint(\"Top 20 popular items (display -> freq):\")\n",
    "for it,f in item_freq.most_common(20):\n",
    "    display = norm_to_display_counts[it].most_common(1)[0][0]\n",
    "    vprint(f\" {display} -> {f}\")\n",
    "\n",
    "# ---------- compute PPMI ----------\n",
    "vprint(\"Computing PPMI...\")\n",
    "total_pairs_weight = sum(item_freq.values())  # use sum of frequencies as approx total mass\n",
    "if total_pairs_weight <= 0:\n",
    "    total_pairs_weight = 1.0\n",
    "\n",
    "item_prob = {it: freq / total_pairs_weight for it, freq in item_freq.items()}\n",
    "ppmi = defaultdict(dict)\n",
    "for a, nbrs in co_counts.items():\n",
    "    for b, co in nbrs.items():\n",
    "        if co < MIN_CO_OCCURRENCE: \n",
    "            continue\n",
    "        p_ab = co / total_pairs_weight\n",
    "        pa = item_prob.get(a, 1e-12)\n",
    "        pb = item_prob.get(b, 1e-12)\n",
    "        raw = math.log((p_ab + PPMI_EPS) / (pa * pb + PPMI_EPS))\n",
    "        v = max(raw, 0.0)\n",
    "        if v > 0:\n",
    "            ppmi[a][b] = v\n",
    "\n",
    "# ---------- helper: display name from norm ----------\n",
    "def display_name(norm):\n",
    "    if norm in norm_to_display_counts and norm_to_display_counts[norm]:\n",
    "        return norm_to_display_counts[norm].most_common(1)[0][0]\n",
    "    return norm.title()\n",
    "\n",
    "# ---------- detect sides/drinks keywords for business rule ----------\n",
    "SIDES_KEYWORDS = ['fries','cheese','onion','corn','salad','coleslaw','rice','sandwich','tenders','tender','nugget']\n",
    "DRINKS_KEYWORDS = ['soda','drink','iced tea','lemonade','pepsi','coke','sprite','soft drink','20 oz','20oz','20 oz soda']\n",
    "\n",
    "def contains_keyword(norm, keyword_list):\n",
    "    s = norm.lower()\n",
    "    for kw in keyword_list:\n",
    "        if kw in s:\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "# ---------- recommendation scorer ----------\n",
    "def recommend_for_cart_display(cart_display_items, top_k=TOP_K):\n",
    "    \"\"\"\n",
    "    cart_display_items: list of the original display strings from test file (not normalized)\n",
    "    returns list of top_k display strings\n",
    "    \"\"\"\n",
    "    # normalize test cart items to our norm keys\n",
    "    cart_norms = []\n",
    "    for d in cart_display_items:\n",
    "        if d is None: continue\n",
    "        dstr = str(d).strip()\n",
    "        if dstr == \"\" or dstr.lower() in {'missing','na','nan'}:\n",
    "            continue\n",
    "        n = dstr.lower()\n",
    "        if n in item_freq:\n",
    "            cart_norms.append(n)\n",
    "        else:\n",
    "            # try to find closest normalized key by exact display match in mapping\n",
    "            # sometimes test items use exact display form used in mapping\n",
    "            found = None\n",
    "            for norm, ctr in norm_to_display_counts.items():\n",
    "                # if display equal to dstr, use that norm\n",
    "                if dstr in ctr:\n",
    "                    found = norm; break\n",
    "            if found:\n",
    "                cart_norms.append(found)\n",
    "    # filter noise\n",
    "    cart_norms = [c for c in cart_norms if not is_noise_item(norm_to_display_counts[c].most_common(1)[0][0])]\n",
    "\n",
    "    # if cart empty or no known items -> fallback to top popular items\n",
    "    if not cart_norms:\n",
    "        out = [ display_name(it) for it,_ in item_freq.most_common(top_k) ]\n",
    "        return out\n",
    "\n",
    "    candidate_scores = defaultdict(float)\n",
    "    # collect candidates from ppmi neighbors\n",
    "    for c in cart_norms:\n",
    "        # prefer neighbors with high PPMI weighted by co-strength\n",
    "        for cand, v in ppmi.get(c, {}).items():\n",
    "            if cand in cart_norms: \n",
    "                continue\n",
    "            co = co_counts[c].get(cand, 0)\n",
    "            # score = ppmi * log1p(co)\n",
    "            candidate_scores[cand] += ALPHA_PPMI * v * math.log1p(co)\n",
    "\n",
    "    # add popularity fallback candidates to ensure we have enough\n",
    "    if len(candidate_scores) < top_k:\n",
    "        for it, _ in item_freq.most_common(500):\n",
    "            if it in cart_norms: continue\n",
    "            candidate_scores.setdefault(it, 0.0)\n",
    "            candidate_scores[it] += GAMMA_POP * item_prob.get(it, 0.0)\n",
    "\n",
    "    if not candidate_scores:\n",
    "        # final fallback\n",
    "        return [ display_name(it) for it,_ in item_freq.most_common(top_k) ]\n",
    "\n",
    "    # business rule: if cart has wings (keyword), promote sides/drinks\n",
    "    promote_sides = any(('wing' in c or 'wings' in c) for c in cart_norms)\n",
    "    # rank candidates\n",
    "    ranked = sorted(candidate_scores.items(), key=lambda x: x[1], reverse=True)\n",
    "    ranked_norms = [it for it,_ in ranked]\n",
    "\n",
    "    if promote_sides:\n",
    "        # move items containing sides/drink keywords up\n",
    "        sides = [it for it in ranked_norms if contains_keyword(it, SIDES_KEYWORDS) or contains_keyword(it, DRINKS_KEYWORDS)]\n",
    "        others = [it for it in ranked_norms if it not in sides]\n",
    "        ranked_norms = sides + others\n",
    "\n",
    "    # build final, avoid duplicates and items in cart\n",
    "    final = []\n",
    "    for n in ranked_norms:\n",
    "        if n in cart_norms: continue\n",
    "        if is_noise_item(norm_to_display_counts[n].most_common(1)[0][0]): \n",
    "            continue\n",
    "        if n not in final:\n",
    "            final.append(n)\n",
    "        if len(final) >= top_k: break\n",
    "\n",
    "    # pad with popular if needed\n",
    "    if len(final) < top_k:\n",
    "        for it, _ in item_freq.most_common(500):\n",
    "            if it in cart_norms or it in final: continue\n",
    "            final.append(it)\n",
    "            if len(final) >= top_k: break\n",
    "\n",
    "    return [ display_name(n) for n in final[:top_k] ]\n",
    "\n",
    "# ------------------ generate recommendations for test file ------------------\n",
    "if not os.path.exists(TEST_CSV):\n",
    "    raise FileNotFoundError(f\"{TEST_CSV} not found.\")\n",
    "\n",
    "test_df = pd.read_csv(TEST_CSV, dtype=str, keep_default_na=False, na_values=[\"\"])\n",
    "# detect item cols\n",
    "test_item_cols = [c for c in test_df.columns if re.match(r'item\\d+', c.strip().lower())]\n",
    "if not test_item_cols:\n",
    "    # try 'item' in name\n",
    "    test_item_cols = [c for c in test_df.columns if 'item' in c.lower()]\n",
    "if not test_item_cols:\n",
    "    raise ValueError(\"Couldn't detect item columns in test file. Expected item1,item2,item3 or similar.\")\n",
    "\n",
    "vprint(\"Detected test item columns:\", test_item_cols)\n",
    "\n",
    "output_rows = []\n",
    "for idx, row in tqdm(test_df.iterrows(), total=len(test_df)):\n",
    "    cart_display = []\n",
    "    for c in test_item_cols:\n",
    "        val = row.get(c, \"\")\n",
    "        if pd.isna(val) or str(val).strip() == \"\": continue\n",
    "        # skip tokens like 'Missing'\n",
    "        if str(val).strip().lower() in {'missing','na','nan'}: continue\n",
    "        cart_display.append(str(val).strip())\n",
    "    recs = recommend_for_cart_display(cart_display, top_k=TOP_K)\n",
    "    out = dict(row)\n",
    "    out['RECOMMENDATION 1'] = recs[0] if len(recs) > 0 else \"\"\n",
    "    out['RECOMMENDATION 2'] = recs[1] if len(recs) > 1 else \"\"\n",
    "    out['RECOMMENDATION 3'] = recs[2] if len(recs) > 2 else \"\"\n",
    "    output_rows.append(out)\n",
    "\n",
    "out_df = pd.DataFrame(output_rows)\n",
    "out_df.to_csv(OUTPUT_CSV, index=False, encoding='utf-8')\n",
    "vprint(\"Wrote\", OUTPUT_CSV)\n",
    "\n",
    "# debug prints\n",
    "vprint(\"Sample recommendations (first 8 rows):\")\n",
    "print(out_df[['RECOMMENDATION 1','RECOMMENDATION 2','RECOMMENDATION 3']].head(8).to_string(index=False))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ef912b7-ccc1-45bc-af8d-8e5492321454",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
